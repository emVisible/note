# 决策树

---

使用树的数据结构进行决策。本质上可以说是从顶点开始向下不断生成的if-else的顺序结构语句。其满足树的定义以及特点，大致分为顶节点，内部节点，叶子节点。

### 特点

- 工业应用多，往往是首选
- 训练简单
- 需要调整超参数少(调参少)
- 结果相对较好
- 拓展性好(数据 && 模型均可升级)

### 应用范围

- **分类** 针对具体的数据item，侧重信息本身
- **回归** 针对具体数字value

### 决策树的好处

最大的好处是可解释

1. **结果和过程能够进行语言解释**

   用于一些对最终结果需要过程 && 原因的行业。如银行，保险等高度涉及计算过程的行业，通知客户服务的达标情况以及给出过程

2. **能够处理数值类 & 类别类的特征**

   数值类，即节点中的值之间的大小关系

   类别类，即节点中的节点属性的值的关系

### 决策树的坏处

最大的问题是不稳定

1. **受噪音十分影响大**

   通常需要集成学习

2. **容易过拟合** 

   数据复杂度和树的规模正相关，当数据本身很复杂的时候，生成的树也十分复杂(精准)，容易过拟合

3. **并行计算难度高**

   树结构对上层的节点依赖，不容易通过GPU并行计算，线上部署会产生性能较差的情况

### 稳定策略

核心是通过多颗树降低偏移和方差

1. **随机森林** Random Forest

   随机并行生成并独立训练多棵树，最后汇总结果。

   类似于民主投票。其核心在于随机，若不是随机，则对决策树的稳定性提升会很有限

   缺点：开销大。n颗树训练 && 预测成本为原来的n倍

   随机性来源(Bagging)：从训练集随机采样(有放回，数据可重复)，即拿到bagging。

   1. 在bagging上重复训练，直至训练完n棵树。
   2. 在bagging上随机采样特征(无放回，不出现重复的列)，使用部分特征)

2. **助推** Boosting(Gradient)

   生成并训练多颗树，但是顺序完成，树之间可合成较大的模型。

   通过计算特征残差(residual, 真实值和预测值之间的差)，在残差之上训练树(在现有树不准的地方继续训练树)，可以达到缩小预测值和真实值之间的差距，即将这颗树和已有的树相加。

   每次训练新的树，拟合负梯度。
   $$
   
   $$